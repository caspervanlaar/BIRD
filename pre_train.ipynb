{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available, training on GPU.\n",
      "Epoch 1/100\n",
      "6114/6114 [==============================] - 627s 101ms/step - loss: 1315.0095 - accuracy: 0.6356 - auc: 0.4915 - top_k_categorical_accuracy: 0.7522\n",
      "Epoch 2/100\n",
      "6114/6114 [==============================] - 596s 97ms/step - loss: 1312.5120 - accuracy: 0.6964 - auc: 0.4927 - top_k_categorical_accuracy: 0.8329\n",
      "Epoch 3/100\n",
      "6114/6114 [==============================] - 617s 101ms/step - loss: 1311.7041 - accuracy: 0.7326 - auc: 0.4963 - top_k_categorical_accuracy: 0.8871\n",
      "Epoch 4/100\n",
      "6114/6114 [==============================] - 576s 94ms/step - loss: 1302.7803 - accuracy: 0.7730 - auc: 0.5200 - top_k_categorical_accuracy: 0.9335\n",
      "Epoch 5/100\n",
      "6114/6114 [==============================] - 574s 94ms/step - loss: 1316.6840 - accuracy: 0.7740 - auc: 0.5197 - top_k_categorical_accuracy: 0.9384\n",
      "Epoch 6/100\n",
      "6114/6114 [==============================] - 590s 97ms/step - loss: 1314.6399 - accuracy: 0.7693 - auc: 0.5207 - top_k_categorical_accuracy: 0.9412\n",
      "Epoch 7/100\n",
      "6114/6114 [==============================] - 599s 98ms/step - loss: 1307.6674 - accuracy: 0.7723 - auc: 0.5166 - top_k_categorical_accuracy: 0.9430\n",
      "Epoch 8/100\n",
      "6114/6114 [==============================] - 607s 99ms/step - loss: 1319.3138 - accuracy: 0.7724 - auc: 0.5190 - top_k_categorical_accuracy: 0.9419\n",
      "Epoch 9/100\n",
      "6114/6114 [==============================] - 584s 95ms/step - loss: 1314.9594 - accuracy: 0.7756 - auc: 0.5200 - top_k_categorical_accuracy: 0.9449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pretrained_autoencoder_2024\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pretrained_autoencoder_2024\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining completed. Model saved as 'pretrained_autoencoder_2024'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, metrics, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# Global Parameters & Paths\n",
    "# ============================\n",
    "SR = 32000\n",
    "DURATION = 5.0\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "N_MELS = 42\n",
    "N_MFCC = 128\n",
    "Patience = 5  # Early stopping patience\n",
    "TIME_STRETCH_RATE = 0.0\n",
    "PITCH_SHIFT_SEMITONES = 0\n",
    "NOISE_LEVEL = 0.01\n",
    "DROPOUT_RATE = 0.1\n",
    "alpha = 0.01\n",
    "frac = 1\n",
    "TOP_K = 3\n",
    "\n",
    "BASE_DIR_2024 = r\"C:\\BIRD_DATA\\2024\"\n",
    "TRAIN_AUDIO_DIR_2024 = os.path.join(BASE_DIR_2024, \"train_audio\")\n",
    "TRAIN_CSV_PATH_2024 = os.path.join(BASE_DIR_2024, \"train_metadata.csv\")\n",
    "\n",
    "# ============================\n",
    "# Load Metadata (train.csv)\n",
    "# ============================\n",
    "train_df_2024 = pd.read_csv(TRAIN_CSV_PATH_2024)\n",
    "sampled_df_2024 = train_df_2024.sample(frac=frac, random_state=42)\n",
    "\n",
    "# ============================\n",
    "# Load 2024 Training Audio File Paths\n",
    "# ============================\n",
    "train_file_paths_2024 = [os.path.join(TRAIN_AUDIO_DIR_2024, fname) for fname in sampled_df_2024[\"filename\"].values]\n",
    "\n",
    "# ============================\n",
    "# Audio Processing Functions (Without Noise Augmentation)\n",
    "# ============================\n",
    "def load_and_process_audio(path, sr=SR, duration=DURATION, n_mels=N_MELS, n_mfcc=N_MFCC, target_length=313):\n",
    "    try:\n",
    "        audio, _ = librosa.load(path, sr=sr, duration=duration)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "        mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel_spectrogram), n_mfcc=n_mfcc)\n",
    "        mfcc = mfcc.T\n",
    "        if mfcc.shape[0] < target_length:\n",
    "            padding = target_length - mfcc.shape[0]\n",
    "            mfcc = np.pad(mfcc, ((0, padding), (0, 0)), mode='constant')\n",
    "        elif mfcc.shape[0] > target_length:\n",
    "            mfcc = mfcc[:target_length, :]\n",
    "        return mfcc\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path}\")\n",
    "        return None\n",
    "    except librosa.LibrosaError as e:\n",
    "        print(f\"Librosa error processing {path}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"General error processing {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================\n",
    "# Data Generator using MFCC Features for Autoencoder\n",
    "# ============================\n",
    "def data_generator_autoencoder(paths, batch_size=BATCH_SIZE):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(paths, size=batch_size)\n",
    "        batch_features = [load_and_process_audio(path) for path in batch_paths]\n",
    "        batch_features = [feature for feature in batch_features if feature is not None]\n",
    "        if len(batch_features) == 0:\n",
    "            continue\n",
    "        batch_features = np.array(batch_features)\n",
    "        yield batch_features, batch_features  # Input and output are the same for autoencoder\n",
    "\n",
    "feature_dim = load_and_process_audio(train_file_paths_2024[0]).shape\n",
    "\n",
    "train_generator_2024 = data_generator_autoencoder(train_file_paths_2024)\n",
    "\n",
    "# ============================\n",
    "# Model Definition (Autoencoder)\n",
    "# ============================\n",
    "def create_autoencoder_mfcc_model(input_shape, dropout_rate=0.0):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    encoded = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Decoder\n",
    "    num_units = input_shape[0] * input_shape[1]\n",
    "    x = layers.Dense(num_units, activation='relu')(encoded)\n",
    "    x = layers.Reshape((input_shape[0], input_shape[1], 1))(x)\n",
    "    decoded = layers.Conv2D(1, kernel_size=(3, 3), padding='same', activation='sigmoid')(x)\n",
    "    decoded = layers.Reshape(input_shape)(decoded)\n",
    "\n",
    "    model = models.Model(inputs, decoded)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.top_k_categorical_accuracy]) #Added auc)  # Only MSE loss\n",
    "    return model\n",
    "\n",
    "model_2024 = create_autoencoder_mfcc_model(((313, 42)))\n",
    "\n",
    "# ============================\n",
    "# Training (Pretraining on 2024 Data)\n",
    "# ============================\n",
    "early_stopping = callbacks.EarlyStopping(monitor='loss', patience=Patience, restore_best_weights=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU available, training on GPU.\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        try:\n",
    "            history = model_2024.fit(\n",
    "                train_generator_2024,\n",
    "                epochs=EPOCHS,\n",
    "                steps_per_epoch=len(train_file_paths_2024) // BATCH_SIZE,\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model.fit() on GPU: {e}\")\n",
    "else:\n",
    "    print(\"GPU not available, training on CPU.\")\n",
    "    try:\n",
    "        history = model_2024.fit(\n",
    "            train_generator_2024,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=len(train_file_paths_2024) // BATCH_SIZE,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model.fit() on CPU: {e}\")\n",
    "\n",
    "# ============================\n",
    "# Saving Pretrained Model\n",
    "# ============================\n",
    "model_2024.save(\"pretrained_autoencoder_2024\")\n",
    "\n",
    "print(\"Pretraining completed. Model saved as 'pretrained_autoencoder_2024'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPIKEDETEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
