{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_2/conv2d_4/Relu' defined at (most recent call last):\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_21888\\209120348.py\", line 220, in <module>\n      history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[early_stopping, adaptive_dropout_callback, reduce_lr])\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\activations.py\", line 318, in relu\n      x, alpha=alpha, max_value=max_value, threshold=threshold\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_2/conv2d_4/Relu'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 0#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 2#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 6#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 5#TC: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 5: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 7#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n\t [[{{node model_2/conv2d_4/Relu}}]] [Op:__inference_train_function_9370]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21888\\209120348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;31m# ============================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madaptive_dropout_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;31m# ============================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'model_2/conv2d_4/Relu' defined at (most recent call last):\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_21888\\209120348.py\", line 220, in <module>\n      history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[early_stopping, adaptive_dropout_callback, reduce_lr])\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\activations.py\", line 318, in relu\n      x, alpha=alpha, max_value=max_value, threshold=threshold\n    File \"c:\\Users\\caspe\\anaconda3\\envs\\SPIKEDETEC\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_2/conv2d_4/Relu'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 0#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 2#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 6#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 5#TC: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 5: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 7#TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n  Profiling failure on CUDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4839): 'status'\n\t [[{{node model_2/conv2d_4/Relu}}]] [Op:__inference_train_function_9370]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output  # For clearing output in Jupyter/Colab\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output  # For clearing output in Jupyter/Colab\n",
    "\n",
    "# ============================\n",
    "# Global Parameters & Paths\n",
    "# ============================\n",
    "SR = 32000\n",
    "DURATION = 5.0\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100  # increased epochs to see the effect of patience.\n",
    "N_MELS = 56\n",
    "N_MFCC = 42\n",
    "patience = 5  # Added patience variable\n",
    "TIME_STRETCH_RATE = 0.1  # Percentage to stretch/compress time\n",
    "PITCH_SHIFT_SEMITONES = 2  # Maximum semitones to shift pitch\n",
    "NOISE_LEVEL = 0.005  # Level of random noise to add\n",
    "DROPOUT_RATE = 0.2  # Added dropout rate.\n",
    "alpha = 0.01\n",
    "\n",
    "BASE_DIR = r\"C:\\BIRD_DATA\"\n",
    "TRAIN_AUDIO_DIR = os.path.join(BASE_DIR, \"train_audio\")\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "\n",
    "# ============================\n",
    "# Load Metadata and Sample 1%\n",
    "# ============================\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Sample 1% of the data\n",
    "sampled_df = train_df.sample(frac=0.01, random_state=42)\n",
    "\n",
    "species_codes = sorted(sampled_df[\"primary_label\"].unique())\n",
    "species_to_index = {code: i for i, code in enumerate(species_codes)}\n",
    "num_species = len(species_codes)\n",
    "\n",
    "# ============================\n",
    "# Audio Processing Functions with Augmentation\n",
    "# ============================\n",
    "def load_and_process_audio(path, sr=SR, duration=DURATION, n_mels=N_MELS, n_mfcc=N_MFCC, target_length=313,\n",
    "                           time_stretch_rate=TIME_STRETCH_RATE, pitch_shift_semitones=PITCH_SHIFT_SEMITONES, noise_level=NOISE_LEVEL):\n",
    "    try:\n",
    "        audio, _ = librosa.load(path, sr=sr, duration=duration)\n",
    "\n",
    "        # Apply Data Augmentation (Randomly)\n",
    "        if np.random.rand() < 0.5:  # 50% chance of applying augmentation\n",
    "            # Time Stretching\n",
    "            if np.random.rand() < 0.3:  # 30% chance of time stretching\n",
    "                rate = np.random.uniform(1.0 - time_stretch_rate, 1.0 + time_stretch_rate)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "\n",
    "            # Pitch Shifting\n",
    "            if np.random.rand() < 0.3:  # 30% chance of pitch shifting\n",
    "                n_steps = np.random.randint(-pitch_shift_semitones, pitch_shift_semitones + 1)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "            # Adding Noise\n",
    "            if np.random.rand() < 0.3:  # 30% chance of adding noise\n",
    "                noise = np.random.randn(len(audio)) * noise_level\n",
    "                audio = audio + noise\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "        mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel_spectrogram), n_mfcc=n_mfcc)\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        if mfcc.shape[0] < target_length:\n",
    "            padding = target_length - mfcc.shape[0]\n",
    "            mfcc = np.pad(mfcc, ((0, padding), (0, 0)), mode='constant')\n",
    "        elif mfcc.shape[0] > target_length:\n",
    "            mfcc = mfcc[:target_length, :]\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return np.zeros((target_length, n_mfcc))\n",
    "\n",
    "# ============================\n",
    "# Label Encoding\n",
    "# ============================\n",
    "def label_to_onehot(label):\n",
    "    onehot = np.zeros(num_species, dtype=np.float32)\n",
    "    if label in species_to_index:\n",
    "        onehot[species_to_index[label]] = 1.0\n",
    "    return onehot\n",
    "\n",
    "train_file_paths = [os.path.join(TRAIN_AUDIO_DIR, fname) for fname in sampled_df[\"filename\"].values]\n",
    "train_labels = [label_to_onehot(lbl) for lbl in sampled_df[\"primary_label\"].values]\n",
    "\n",
    "train_paths, val_paths, train_lab, val_lab = train_test_split(train_file_paths, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ============================\n",
    "# Data Generator using MFCC Features with Augmentation\n",
    "# ============================\n",
    "def generator(paths, labels):\n",
    "    for path, label in zip(paths, labels):\n",
    "        mfcc = load_and_process_audio(path)\n",
    "        yield mfcc, label\n",
    "\n",
    "feature_dim = load_and_process_audio(train_paths[0]).shape\n",
    "output_shapes = (feature_dim, (num_species,))\n",
    "output_types = (tf.float32, tf.float32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: generator(train_paths, train_lab), output_types=output_types, output_shapes=output_shapes).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_generator(lambda: generator(val_paths, val_lab), output_types=output_types, output_shapes=output_shapes).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Adaptive Dropout Class\n",
    "# ============================\n",
    "class AdaptiveDropout(callbacks.Callback):\n",
    "    def __init__(self, model, initial_dropout=0.3, min_dropout=0.05, max_dropout=0.5,\n",
    "                 scaling_factor=0.5, baseline_gap=0.05):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dropout_rate = initial_dropout\n",
    "        self.min_dropout = min_dropout\n",
    "        self.max_dropout = max_dropout\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.baseline_gap = baseline_gap\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        train_acc = logs.get('accuracy', 0)\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "\n",
    "        if train_acc > 0:\n",
    "            relative_gap = (train_acc - val_acc) / train_acc\n",
    "            powered_gap = relative_gap**2\n",
    "        else:\n",
    "            relative_gap = 0\n",
    "\n",
    "        if powered_gap > self.baseline_gap:\n",
    "            percent_change = self.scaling_factor * (((powered_gap)**.5 - self.baseline_gap) / self.baseline_gap)\n",
    "            new_dropout = self.dropout_rate * (1 + percent_change)            \n",
    "        else:\n",
    "            percent_change = self.scaling_factor * ((self.baseline_gap - relative_gap) / self.baseline_gap)\n",
    "            new_dropout = self.dropout_rate * (1 - percent_change)\n",
    "\n",
    "        new_dropout = min(max(new_dropout, self.min_dropout), self.max_dropout)\n",
    "\n",
    "        self.dropout_rate = new_dropout\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, layers.Dropout):\n",
    "                layer.rate = self.dropout_rate\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Training Acc = {train_acc:.4f}, Validation Acc = {val_acc:.4f}, \"\n",
    "              f\"Relative Gap = {relative_gap:.4f}, New Dropout = {self.dropout_rate:.4f}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# WaveNet-cnn like Model Definition with Leaky ReLU and Dropout\n",
    "# ============================\n",
    "def create_wavenet_model(input_shape, num_classes, dropout_rate=DROPOUT_RATE):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # CNN Layers for Spectral Feature Extraction\n",
    "    cnn = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(layers.Reshape((input_shape[0], input_shape[1], 1))(inputs))\n",
    "    cnn = layers.MaxPooling2D((2, 2))(cnn)\n",
    "    cnn = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(cnn)\n",
    "    cnn = layers.MaxPooling2D((2, 2))(cnn)\n",
    "    cnn = layers.Reshape((-1, 64 * (input_shape[1] // 4)))(cnn) #Flattening for input into 1D conv layers\n",
    "\n",
    "    # WaveNet-like Layers for Temporal Feature Extraction\n",
    "    dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "    filters = 64\n",
    "    activation = 'leaky_relu'\n",
    "\n",
    "    x = layers.Conv1D(32, 3, dilation_rate=1, padding='causal')(cnn)\n",
    "    x = layers.LeakyReLU(alpha=alpha)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    for dilation in dilations:\n",
    "        x = layers.Conv1D(filters, 3, dilation_rate=dilation, padding='causal')(x)\n",
    "        x = layers.LeakyReLU(alpha=alpha)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters, 3, dilation_rate=512, padding='causal')(x)\n",
    "    x = layers.LeakyReLU(alpha=alpha)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================\n",
    "# Model Instantiation\n",
    "# ============================\n",
    "model = create_wavenet_model(((313, 42)), num_species)\n",
    "adaptive_dropout_callback = AdaptiveDropout(model)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# ============================\n",
    "# Training\n",
    "# ============================\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[early_stopping, adaptive_dropout_callback, reduce_lr])\n",
    "\n",
    "# ============================\n",
    "# Plot Training History\n",
    "# ============================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# Saving Model\n",
    "# ============================\n",
    "model.save(\"WAVENET256_Species_adaptive_dropout\")\n",
    "print(\"Model training complete. Saved as WAVENET256_Species_adaptive_dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPIKEDETEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
